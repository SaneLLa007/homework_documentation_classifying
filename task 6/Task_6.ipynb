{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLy2I8x-bQEk"
   },
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тестовое множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX7EjZ-bbQE0"
   },
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsu7EnvVhjeq",
    "outputId": "b1639d8c-2b1c-4d32-f0b2-59fc13c7cad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugiXr1-Osl9e",
    "outputId": "28647994-ec90-4316-d696-6f82b8757eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iM6zADOzhnlv",
    "outputId": "fb89daee-ae4f-462b-b81e-6dabfa81f068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9nm545MT2E5q"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/content/drive/MyDrive/news_train.txt\", sep=\"\\t\", header=None, names=['topic', 'headline', 'info'])\n",
    "test = pd.read_csv(\"/content/drive/MyDrive/news_test.txt\", sep=\"\\t\", header=None, names=['topic', 'headline', 'info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qgMaPLADaNTd"
   },
   "outputs": [],
   "source": [
    "#encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['topic'])\n",
    "test['label'] = label_encoder.transform(test['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cxfoeP1RKMZQ"
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # delete punctuation\n",
    "    punctuation = string.punctuation + '«' + '»' + '—'\n",
    "    text = \"\".join(ch if ch not in punctuation else ' ' for ch in text)\n",
    "    # delete digits\n",
    "    text = \"\".join([ch if not ch.isdigit() else ' ' for ch in text])\n",
    "    #lemmatization\n",
    "    words = text.split()\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        word = morphy.parse(word)[0].normal_form;\n",
    "        #delete stop words \n",
    "        if word not in stopwords.words(\"russian\"):\n",
    "            tokens.append(word)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5XUQxq_TSQX"
   },
   "outputs": [],
   "source": [
    "morphy = pymorphy2.MorphAnalyzer()\n",
    "for i in range(len(train)):\n",
    "    train[\"info\"][i] = process_text(train[\"info\"][i])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    test[\"info\"][i] = process_text(test[\"info\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDFn7Y5UZFF_",
    "outputId": "f3519479-36a4-4c74-9801-8d796aa7dcb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('игровой', 0.6991193890571594),\n",
       " ('серия', 0.693912148475647),\n",
       " ('ведьмак', 0.6892495155334473),\n",
       " ('китамур', 0.6836866140365601),\n",
       " ('консоль', 0.6814697980880737),\n",
       " ('дополнение', 0.6809597015380859),\n",
       " ('dlc', 0.6758855581283569),\n",
       " ('pc', 0.6745489835739136),\n",
       " ('геймер', 0.6738331913948059),\n",
       " ('dota', 0.6719385385513306)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec\n",
    "words = [text.split() for text in train[\"info\"]]\n",
    "model = Word2Vec(sentences=words, min_count=0)\n",
    "model.wv.most_similar('игра')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xV68vg_R9Ipn"
   },
   "outputs": [],
   "source": [
    "#vectoriation\n",
    "vectorizer = TfidfVectorizer(min_df = 10, max_df = 1., max_features = 10000)\n",
    "train_vectors = vectorizer.fit_transform(train[\"info\"]).toarray()\n",
    "test_vectors = vectorizer.transform(test[\"info\"]).toarray()\n",
    "train_labels = np.array(train['label'])\n",
    "test_labels = np.array(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNUBB6WVjKAA",
    "outputId": "fbb101d4-4eb0-4a24-e42b-beaf2b0de52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.882\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "model = SVC(kernel='linear', C=1, gamma = 0.01)\n",
    "model.fit(train_vectors, train_labels)\n",
    "print('accuracy: ', accuracy_score(model.predict(test_vectors), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y62etwZLzDZp",
    "outputId": "bf00cd71-7a39-41d5-f510-32177b22e263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8456666666666667\n"
     ]
    }
   ],
   "source": [
    "#Multinomial NB\n",
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(train_vectors, train_labels)\n",
    "print('accuracy: ', accuracy_score(model.predict(test_vectors), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
